[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Modelling",
    "section": "",
    "text": "These coures notes are made available under the Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "1  Principal component analysis",
    "section": "",
    "text": "In the previous chapter we built a linear model to predict the amount of body fat, given measurements of the thickness of the triceps skin fold, the thigh circumference, and the midarm circumference. We saw that the dataset used for this model suffers from multicollinearity, meaning that some of the predictors (or linear combinations of predictors) are correlated with one another. Intuitively speaking, multicollinearity means that some variables don’t contribute much to the expressivity of the data: we could omit them and end up with a dataset that is almost as informative as the original one.\nTo find out which combinations of variables contribute most to the variability of our data, we will turn to principal component analysis, one of the mainstays of statistical data analysis. Principal component analysis will allow us to identify the main sources of variability in our dataset, and will tell us which combinations of variables can be omitted with only little impact on the data itself. This allows us to reduce the number of features in the dataset, and makes principal component analysis into what is called a technique for dimensionality reduction. This is useful for a number of reasons:\nThese coures notes are made available under the Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "pca.html#intuition",
    "href": "pca.html#intuition",
    "title": "1  Principal component analysis",
    "section": "1.1 Intuition",
    "text": "1.1 Intuition\nPrincipal component analysis (PCA) finds a low-dimensional approximation to the dataset which retains as much as possible the variability of the original dataset. To understand what is meant by this, let’s revisit the body fat dataset of chapter 1. In this dataset, the features are the measurements of the thickness of the triceps skin fold, the thigh circumference, and the midarm circumference. The total body fat is the outcome, but we will not consider it for the time being.\n\n\nCode\nbodyfat <- read.csv(\"datasets/02-PCA/bodyfatNKNW.txt\", sep = \" \")\nbodyfat_predictors <- bodyfat[,c(\"triceps.skinfold.thickness\", \"thigh.circumference\", \"midarm.circumference\")]\n\nlibrary(GGally)\nggpairs(bodyfat_predictors)\n\n\n\n\n\nCorrelations between the different variables in the body fat dataset. The variable triceps.skinfold.thickness is strongly correlated with the other two variables in the dataset and hence carries less information.\n\n\n\n\nFrom the scatter matrix, we see that triceps.skinfold.thickness and thigh.circumference are highly correlated: if you know one, you can predict the other one reasonably well. This makes it feel like a waste to analyze both: since they carry the same information, we can just as well throw one or the other away, or replace both by a linear combination. Let’s do the latter, and introduce a new variable z1 which is the sum of both. In terms of this variable and midarm.circumference, which we leave unchanged, the dataset has only two features that are mildly correlated, as shown on the scatter plot below.\n\n\nCode\np <- with(bodyfat_predictors, {\n  z1 <- triceps.skinfold.thickness + thigh.circumference\n  df <- data.frame(\n    z1 = z1, midarm.circumference = midarm.circumference\n  )\n  ggpairs(df)\n})\np\n\n\n\n\n\nWe have succeeded in our aim to reduce the number of features in our dataset from 3 to 2, but a number of questions immediately pop up:\n\nWhat is the meaning of the z1 variable, and can we find it without looking at a scatter plot?\nHow much information do we lose by considering two variables instead of three? Will the conclusions from the reduced dataset still be valid for the full dataset?\n\nIt will turn out that the variable z1, which we constructed in an ad-hoc way, is remarkably close to the first principal component of the dataset, and we will discover a way to compute all principal components. We will also see that by discarding more or fewer principal components, as much variability of the original dataset can be retained as is needed."
  },
  {
    "objectID": "pca.html#derivation-of-the-pca-algorithm",
    "href": "pca.html#derivation-of-the-pca-algorithm",
    "title": "1  Principal component analysis",
    "section": "1.2 Derivation of the PCA algorithm",
    "text": "1.2 Derivation of the PCA algorithm\nPCA works by making linear combinations of the original variables so that the total amount of variation is maximal. There is another way of computing principal components, by minimizing the reconstruction error, and it can be shown that both approaches give the same principal components (Bishop (2006), section 12). In this course, we will develop the first method further.\nWe assume that we have \\(N\\) observations \\(\\mathbf{x}_{i}\\), where each \\(\\mathbf{x}_i\\) is a column vector in \\(\\mathbb{R}^D\\). We assemble these observations into an \\(N \\times D\\) data matrix \\(\\mathbf{X}\\), where each row of \\(\\mathbf{X}\\) is an observation \\(\\mathbf{x}_i\\): \\[\n    \\mathbf{X} = \\begin{bmatrix}\n        \\mathbf{x}_1^T \\\\\n        \\mathbf{x}_2^T \\\\\n        \\cdots \\\\\n        \\mathbf{x}_N^T \\\\\n    \\end{bmatrix}.\n\\] Keep in mind that the columns of \\(\\mathbf{X}\\) are the features (also referred to as independent variables or predictors) of the dataset. In the case of the body fat dataset, \\(\\mathbf{X}\\) is a \\(20 \\times 3\\) matrix, since there are 20 observations, with 3 features for each observation. We will refer to the \\(j\\)th column of \\(\\mathbf{X}\\) by the notation \\(\\mathbf{X}_j\\), where \\(j = 1, \\ldots, D\\). Note that \\(\\mathbf{X}_j\\) is a column vector, with \\(N\\) entries, and there are \\(D\\) such columns. For the body fat dataset, the columns correspond to the following features:\n\n\\(\\mathbf{X}_1\\): triceps.skinfold.thickness\n\\(\\mathbf{X}_2\\): thigh.circumference\n\\(\\mathbf{X}_3\\): midarm.circumference\n\n\n1.2.1 The first principal component\nThe first principal component, \\(\\mathbf{Z}_1\\) is a linear combination of the features, so that the amount of variation in \\(\\mathbf{Z}_1\\) is maximized. Let’s unpack these ideas one at a time. The fact that \\(\\mathbf{Z}_1\\) is a linear combination means that it can be written as \\[\n  \\mathbf{Z}_1 = v_1 \\mathbf{X}_1 + \\cdots + v_D \\mathbf{X}_D = \\sum_{j = 1}^D v_j \\mathbf{X}_j.\n\\] where the \\(v_j\\) are coefficients that we have to determine. These coefficients are sometimes referred to as the principal component loadings, since \\(v_j\\) expresses how much of \\(\\mathbf{X}_j\\) is added (“loaded”) to the principal component.\nWe can write this expression for \\(\\mathbf{Z}_1\\) in a compact way by assembling all the coefficients \\(v_j\\) into a vector \\(\\mathbf{v}\\), called the loadings vector: \\[\n  \\mathbf{v} =\n    \\begin{bmatrix}\n      v_1 \\\\\n      v_2 \\\\\n      \\cdots \\\\\n      v_D\n    \\end{bmatrix}.\n\\] With this expression, \\(\\mathbf{Z}_1\\) can be written as a matrix-vector product: \\[\n  \\mathbf{Z}_1 = \\mathbf{X} \\mathbf{v}.\n\\tag{1.1}\\] We will often use this way of expressing \\(\\mathbf{Z}_1\\) as a matrix-vector product, since it makes subsequent calculations easier.\nBefore we go on to determine the loadings \\(v_j\\), let’s focus on the geometry behind Equation 1.1. Each component of \\(\\mathbf{Z}_1\\) can written as \\[\n  (\\mathbf{Z}_1)_i = \\mathbf{x}_i^T \\mathbf{v}.\n\\] This is the dot product of the \\(i\\)th observation \\(\\mathbf{x}_i\\) with the loadings vector \\(\\mathbf{v}\\). This dot product tells us how much of the vector \\(\\mathbf{x}_i\\) is parallel to \\(\\mathbf{v}\\), as shown in Figure 1.1. For example, a data point that is at right angles to \\(\\mathbf{v}\\) will have dot product 0 (no component at all along \\(\\mathbf{v}\\)), while one that is parallel to \\(\\mathbf{v}\\) will have a dot product that is maximal in magnitude.\n\n\n\nFigure 1.1: The \\(i\\)th component of the first principal component \\(\\mathbf{Z}_1\\) is the length of the orthogonal projection of \\(\\mathbf{x}_i\\) onto the line in the direction of \\(\\mathbf{v}\\).\n\n\nIn other words, we obtain \\(\\mathbf{Z}_1\\) by taking a fixed vector \\(\\mathbf{v}\\) and projecting all of our data points on the line through the origin in the direction of \\(\\mathbf{v}\\). If we choose another vector, \\(\\mathbf{w}\\), we obtain a different projection, as indicated on Figure 1.2.\n\n\n\nFigure 1.2: Two different projections, onto the loadings vector \\(\\mathbf{v}\\) (blue) and \\(\\mathbf{w}\\) (red).\n\n\nOur goal is now to find the loadings vector \\(\\mathbf{v}\\) so that the variance of the projected dataset is maximal. To make this problem well-posed, we will assume that \\(\\mathbf{v}\\) has unit norm: \\[\n  \\mathbf{v}^T \\mathbf{v} = 1.\n\\tag{1.2}\\] If we did not impose this constraint, we could increase the amount of variance simply by making \\(\\mathbf{v}\\) longer.\nThe mean of the projected data is given by \\[\n  \\bar{\\mathbf{Z}}_1 = \\frac{1}{N}\\sum_{j=1}^N (\\mathbf{Z}_1)_j\n         = \\frac{1}{N}\\sum_{j=1}^N \\mathbf{x}_j^T \\mathbf{v}\n         = \\mathbf{\\bar{x}}^T \\mathbf{v},\n\\] where \\(\\mathbf{\\bar{x}}\\) is the (sample) mean of the original data points. In other words, the mean \\(\\bar{\\mathbf{Z}}_1\\) is just the mean \\(\\bar{\\mathbf{x}}\\) of the data points, projected onto \\(\\mathbf{v}\\).\nThe variance of the projected data is given by \\[\n    \\sigma^2\n    =\n    \\frac{1}{N} \\sum_{i=1}^N\\left(\n      (\\mathbf{Z}_1)_i - \\bar{\\mathbf{Z}}_1\\right)^2\n     = \\frac{1}{N} \\sum_{i = 1}^N\\left(\n        \\mathbf{x}_i^T \\mathbf{v} -\n        \\mathbf{\\bar{x}}^T \\mathbf{v}\\right)^2.\n\\] This expression can be rewritten as a matrix product: \\[\n  \\sigma^2 = \\mathbf{v}^T \\mathbf{S} \\mathbf{v},\n\\tag{1.3}\\] where \\(\\mathbf{S}\\) is the covariance matrix, given by \\[\n    \\mathbf{S} = \\frac{1}{N}\n      \\sum_{i = 1}^N \\left(\n      \\mathbf{x}_i \\mathbf{x}_i^T -\n      \\mathbf{\\bar{x}}\\mathbf{\\bar{x}}^T \\right).\n\\tag{1.4}\\]\nWe are now ready to translate our problem into a mathematical form, so that we can solve it. To find the first principal component \\(\\mathbf{Z}_1\\), we want to find a loadings vector \\(\\mathbf{v}\\) so that the projected variance \\(\\sigma^2\\), given in Equation 1.3, is maximized. In addition, we want \\(\\mathbf{v}\\) to have unit length, as in Equation 1.2. In mathematical terms: \\[\n  \\mathbf{v} = \\operatorname{argmax} \\mathbf{v}^T \\mathbf{S} \\mathbf{v}, \\quad\n  \\text{such that $\\mathbf{v}^T \\mathbf{v} = 1$}.\n\\]\nWe can solve this optimization problem using the theory of Lagrange multipliers. If we introduce a Lagrange multiplier \\(\\lambda\\) for the unit-length constraint, then the desired vector \\(\\mathbf{v}\\) is given by \\[\n  \\mathbf{v} = \\operatorname{argmax} L(\\mathbf{v})\n\\] where \\(L\\) is given by \\[\n    L(\\mathbf{v}) = \\mathbf{v}^T \\mathbf{S} \\mathbf{v} - \\lambda( \\mathbf{v}^T\\mathbf{v} - 1).\n\\]\nA necessary condition for \\(\\mathbf{v}\\) to be a maximum of \\(L\\) is that the gradient vanishes at \\(\\mathbf{v}\\). Taking the gradient of \\(L\\) with respect to \\(\\mathbf{v}\\) and setting the resulting expression equal to zero gives \\[\n    \\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v}.\n\\tag{1.5}\\] This is a very important result: it tells us that the \\(\\mathbf{v}\\) we are looking for is an eigenvector of the matrix \\(\\mathbf{S}\\), with corresponding eigenvalue \\(\\lambda\\). This will hold true generally, not just for the first principal component: finding the principal components of a data set will involve solving an eigenvalue problem, and selecting the largest eigenvalues.\nLast, we have to find the Lagrange multiplier \\(\\lambda\\). This can be done by multiplying Equation 1.5 from the left by \\(\\mathbf{v}^T\\) to get \\[\n    \\mathbf{v}^T \\mathbf{S} \\mathbf{v} = \\lambda \\mathbf{v}^T \\mathbf{v} = \\lambda,\n\\] where we have used the unit-length constraint Equation 1.2.\nWe see that the Lagrange multiplier \\(\\lambda\\) is precisely the variance \\(\\sigma^2\\) of the first principal component \\(\\mathbf{Z}_1\\). For this reason, we will refer to the eigenvalue \\(\\lambda\\) as the amount of retained variance, since it expresses how much variance is captured by projecting the entire dataset onto the direction \\(\\mathbf{v}\\).\nTo sum up, the first principal component \\(\\mathbf{Z}_1\\) is a linear combination of the original features (columns) of our dataset, chosen so that the variance of \\(\\mathbf{Z}_1\\) is maximal. We can find \\(\\mathbf{Z}_1\\) by looking for the largest eigenvalue \\(\\lambda\\) of the covariance matrix, with unit length eigenvector \\(\\mathbf{v}\\), and projecting the data matrix \\(\\mathbf{X}\\) onto \\(\\mathbf{v}\\).\n\n\n1.2.2 The remaining principal components\nNow that we’ve computed the first principal component, how do we compute the others? It probably won’t come as a surprise that the next principal components, \\(\\mathbf{Z}_2\\), \\(\\mathbf{Z}_3\\), and so on, involve the amount of variation that is left in the data after \\(\\mathbf{Z}_1\\) has been removed, and that they involve the second, third, … largest eigenvalues.\nAssuming that we have computed \\(\\mathbf{Z}_1\\) as in the previous section, and denote the loadings vector by \\(\\mathbf{v}_1\\). Recall that \\(\\mathbf{v}_1\\) points in the direction of the largest variance.\nTo find the next principal component, we consider the variability in the dataset that is not already accounted for by \\(\\mathbf{Z}_1\\). More precisely, we look for a loadings vector \\(\\mathbf{v}_2\\) which is orthogonal to \\(\\mathbf{v}_1\\), has unit length, and maximizes the amount of variability \\(\\mathbf{v}_2^T \\mathbf{S} \\mathbf{v}_2\\). By a similar reasoning as in the previous section, one can show that this \\(\\mathbf{v}_2\\) is an eigenvector associated with the second largest eigenvalue of \\(\\mathbf{S}\\). The projection of the dataset onto this loadings vector then gives us the second principal component: \\[\n  \\mathbf{Z}_2 = \\mathbf{X}\\mathbf{v}_2.\n\\] This procedure can be applied to find all \\(D\\) principal components and results in the following algorithm to compute the principal components:\n\nCompute the sample covariance matrix \\(\\mathbf{S}\\) using Equation 1.4.\nCompute the eigenvalues of \\(\\mathbf{S}\\) and order them from largest to smallest: \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_D\\).\nFind the corresponding eigenvectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_D\\) and normalize them to unit length, if necessary. These vectors are the loading vectors and they point in the directions of highest variance.\nProject the dataset onto the loading vectors to obtain the principal components \\(\\mathbf{Z}_1, \\mathbf{Z}_2, \\ldots, \\mathbf{Z}_D\\).\n\nTypically, we do not have to compute the principal components by hand: most data analysis packages will do this for us, either via a builtin command, such as in R (prcomp) or minitab, or via an extra package, such as scikit-learn (Python) or xlstat (Excel). It is instructive, however, to know the principles behind PCA, so that you can interpret and understand the results.\n\n\n\n\n\n\nNote\n\n\n\nSoftware packages that compute the principal components of a dataset typically do not compute the eigenvalues and eigenvectors of the covariance matrix, despite our derivation above. Instead, they rely on the so-called singular value decomposition (SVD) of the data matrix (after centering). The SVD is typically more accurate and easier to compute, and the principal components obtained in this way agree with the ones computed using the eigendecomposition (to within numerical roundoff).\n\n\n\n\n1.2.3 Worked-out example\nFor this example, we will calculate the principal components in two different ways. We will first compute the principal components by hand, by solving an eigenvalue problem. This is possible because the data are two-dimensional, and solving the characteristic equation for a two- or three-dimensional matrix can be done by hand. This is not practical for real-world datasets, which often contains dozens, thousands, or millions of features, and we will therefore also cover computing the principal components using R.\nOur dataset consists of 100 observations, where each observation has two components. The dataset is shown below and has been carefully constructed so that the covariance matrix \\(\\mathbf{S}\\) and mean \\(\\mathbf{\\bar{x}}\\) are exactly equal to \\[\n    \\mathbf{S} =\n        \\begin{bmatrix}\n            5 & 2 \\\\\n            2 & 2 \\\\\n        \\end{bmatrix},\n      \\quad \\text{and} \\quad\n      \\mathbf{\\bar{x}} =\n        \\begin{bmatrix}\n          1 \\\\\n          1\n        \\end{bmatrix}.\n\\tag{1.6}\\]\n\n\nCode\nlibrary(mvtnorm)\n\nS <- matrix(c(5, 2,  \n              2, 2), byrow = TRUE, nrow = 2)\n\nadjust_cov_mean <- function(z, mean, sigma) {\n  # Adjust NxD matrix z so that sample mean and sample \n  # covariance are exactly `mean` and `sigma`\n  \n  # whiten the z's\n  z_mean <- colMeans(z)\n  z <- t(apply(z, 1, function(row) row - z_mean))\n  R <- chol(cov(z))\n  z <- t(solve(t(R), t(z)))\n\n  # impose desired covariance, mean\n  R <- chol(S)\n  z <- t(apply(z %*% R, 1, function(row) row + mean))\n  z\n}\n\nz <- rmvnorm(100, mean = c(0, 0))\nz <- adjust_cov_mean(z, c(1, 1), S)\ndf <- as.data.frame(z)\ncolnames(df) <- c(\"X\", \"Y\")\n\nggplot(df, aes(x = X, y = Y)) +\n  geom_point()\n\n\n\n\n\nTo find the loading vectors, we must find the eigenvalues of \\(\\mathbf{S}\\), which we can do via the characteristic equation: \\[\n  \\det(\\mathbf{S} - \\lambda \\mathbf{I}) = 0.\n\\]\nSubstituting the expression given in Equation 1.6 for the covariance matrix and expanding the determinant gives \\[\n  \\det \\begin{bmatrix}\n            5 - \\lambda & 2 \\\\\n            2 & 2 - \\lambda \\\\\n        \\end{bmatrix} = (5 - \\lambda)(2 - \\lambda) - 4 = 0.\n\\] The roots of this equation are \\(\\lambda_1 = 6\\) and \\(\\lambda_2 = 1\\). The corresponding eigenvectors are \\[\n    \\mathbf{v}_1 = \\frac{1}{\\sqrt{5}} \\begin{bmatrix}\n        2 \\\\ 1\n    \\end{bmatrix}, \\quad\n    \\mathbf{v}_2 = \\frac{1}{\\sqrt{5}} \\begin{bmatrix}\n        -1 \\\\ 2\n    \\end{bmatrix}.\n\\] These are our loading vectors, and they indicate the direction in which the data varies the most (for \\(\\mathbf{v}_1\\)) and the “second-most” (for \\(\\mathbf{v}_2\\)). Figure Figure 1.3 shows the dataset again, now with the two loading vectors superimposed. Each loading vector has been rescaled by multiplying it with the square root of the corresponding eigenvalue. Why the square root? The eigenvalue itself represents the variance in that direction, the square root the standard deviation.\n\n\nCode\nvx <- 2/5^0.5\nvy <- 1/5^0.5\nsegment <- function(lx, ly, color = \"cornflowerblue\") {\n  geom_segment(aes(x=1, y=1, xend=1 + lx, yend=1 + ly), \n               arrow = arrow(length=unit(0.5, 'cm')),\n               color = color, lwd = 1.5, alpha = 0.8, \n               lineend = \"round\")\n}\n\nl1_sqrt <- 6**0.5\nl2_sqrt <- 1\n\nggplot() +\n  geom_point(data = df, aes(x = X, y = Y)) +\n  segment(l1_sqrt*vx, l1_sqrt*vy, color = \"cornflowerblue\") +\n  segment(-l2_sqrt*vy, l2_sqrt*vx, color = \"chocolate\") + \n  xlim(c(-5, 6)) + ylim(c(-2.5, 4.5))\n\n\n\n\n\nFigure 1.3: The dataset with the first loading vector (blue) and the second loading vector (orange) superimposed. Each loading vector has been rescaled by the square root of the corresponding eigenvalue, to give an indication of the variability in that direction.\n\n\n\n\nTo compute the principal components directly via R, we can use the prcomp command, as shown below. This is generally the preferred option over computing the eigenvalue decomposition by hand: prcomp is more flexible (allowing one, for example, to scale the data before computing the principal components) and is also numerically more stable.1 For our simple dataset, the end result is the same:\n\npca <- prcomp(df)\npca\n\nStandard deviations (1, .., p=2):\n[1] 2.44949 1.00000\n\nRotation (n x k) = (2 x 2):\n        PC1        PC2\nX 0.8944272 -0.4472136\nY 0.4472136  0.8944272\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that prcomp returns (among other things) the standard deviations, which are the square roots of the variances (eigenvalues). To compare the output of prcomp with the results of the eigenvalue analysis, make sure to take the square of the standard deviations, and you will see the eigenvalues appear:\n\npca$sdev^2\n\n[1] 6 1\n\n\n\n\n\n\n1.2.4 Standardizing the predictors\nPrior to doing principal component analysis, the data are often standardized by subtracting the mean for each feature and dividing by the standard deviation. If the original predictors in our dataset are given by \\(\\mathbf{X}_i\\), \\(i = 1, \\ldots, D\\), this means that we introduce standardized variables \\(\\mathbf{Y}_i\\), given by \\[\n  \\mathbf{Y}_i = \\frac{\\mathbf{X}_i - \\bar{\\mathbf{X}}_i}{\\sqrt{S^2_i}},\n\\] where \\(S^2_i\\) is the variance of \\(\\mathbf{X}_i\\). The resulting variables \\(\\mathbf{Y}_i\\) will have mean 0 and variance 1.\nStandardizing the predictors means that they will be comparable in magnitude: variables whose variance is small will gain in importance and large variables will decrease in importance, roughly speaking. This may change the PCA output significantly!\nAs a rule of thumb, you should standardize variables that are measured in different units (e.g. seconds, meters, Watt, …), since the unit can be rescaled without affecting the physical meaning of the variable, or its relation to other variables (e.g., rescaling a variable expressed in meters by a factor of 1000 is the same as expressing that variable in kilometers). By contrast, variables that are measured in the same units or that are unitless should not be rescaled (or should be rescaled collectively).\nFor an example of the latter, think about pixel intensities in an image dataset (such a dataset is in fact analyzed in Section 2.2). Pixels near the edge of the image presumably are part of the background, don’t vary that much, and are relatively unimportant, whereas pixels in the center are likely to be part of the image subject, vary a lot, and carry a lot of information. Standardizing the pixels would make the pixels near the edge just as variable as the pixels in the center, which would greatly amplify the noise in the image at the expense of the useful information in it!\nStandardizing the predictors is also referred to as scaling."
  },
  {
    "objectID": "pca.html#interpreting-the-pca-results",
    "href": "pca.html#interpreting-the-pca-results",
    "title": "1  Principal component analysis",
    "section": "1.3 Interpreting the PCA results",
    "text": "1.3 Interpreting the PCA results\nIn this section we will discuss a number of useful results that follow from PCA. We will use the bodyfat dataset as an illustration throughout.\n\npc <- prcomp(bodyfat_predictors)\npc\n\nStandard deviations (1, .., p=3):\n[1] 7.2046011 3.7432587 0.1330841\n\nRotation (n x k) = (3 x 3):\n                                 PC1        PC2        PC3\ntriceps.skinfold.thickness 0.6926671  0.1511979  0.7052315\nthigh.circumference        0.6985058 -0.3842734 -0.6036751\nmidarm.circumference       0.1797272  0.9107542 -0.3717862\n\n\nWe can also ask R to print a summary of the principal component analysis for us. This will give us a table with the proportion of variance explained by each principal component, as well as the cumulative proportion (the amount of variance retained by that principal component and all previous ones).\n\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2     PC3\nStandard deviation     7.2046 3.7433 0.13308\nProportion of Variance 0.7872 0.2125 0.00027\nCumulative Proportion  0.7872 0.9997 1.00000\n\n\n\n1.3.1 The score plot\nPerhaps the most useful visualization of the PCA is the so-called score plot, which is nothing but a scatter plot of the first two principal components. It often happens that the score plot is sufficient to discern patterns in the data, such as clusters.\nFigure 1.4 shows a score plot for the bodyfat dataset. While no obvious patterns in this dataset stand out, the plot does show that the principal components are uncorrelated, and this is a good confirmation of what we already know on theoretical grounds. It is customary to put the percentages of variance explained on the axis labels of the score plot, so that the person interpreting it can have an idea of how well the first two principal components describe the data.\n\n\nCode\n# Explained variation for first and second component\ntotal_var <- sum(pc$sdev^2)\npct_var_1 <- pc$sdev[1]^2 / total_var\npct_var_2 <- pc$sdev[2]^2 / total_var\n\ndf <- data.frame(PC1 = pc$x[,1], PC2 = pc$x[,2])\nggplot(df, aes(x = PC1, y = PC2)) +\n  geom_point() +\n  xlab(paste0(\"PC1 (\", round(pct_var_1 * 100, 2), \"% var. explained)\")) +\n  ylab(paste0(\"PC2 (\", round(pct_var_2 * 100, 2), \"% var. explained)\"))\n\n\n\n\n\nFigure 1.4: Score plot for the body fat dataset.\n\n\n\n\nAs an aside, above we noted that the principal components are uncorrelated with one another. We can also verify that this is the case numerically. The result is not exactly zero, but it is very small:\n\ncov(pc$x[,1], pc$x[,2])\n\n[1] -2.604824e-16\n\n\n\n\n1.3.2 The loadings plot\nA loadings plot shows the relations of the original variables and the principal components. This is often a useful visualization to see how each variable contributes to the principal components. Moreover, one can show that the loadings are proportional to the Pearson correlations between the principal components and the variables. Hence, if a loading is positive (negative), that variable will be positively (negatively) correlated with that principal component.2\nAt the beginning of this chapter we hypothesized that the variables thigh.circumference and triceps.skinfold.thickness would contribute about equally to the first principal component. From Figure 1.5 we see that this is indeed the case: both variables have loadings approximately equal to \\(0.7\\), when we consider the first principal component. We also see that the second principal component is mostly made up of the variable midarm.circumference.\nUnfortunately there is no command in base R or ggplot to create a loadings plot – you have to make one yourself.\n\n\nCode\nlibrary(tidyr)\nlibrary(tibble)\n\npc <- prcomp(bodyfat_predictors)\ndf <- as.data.frame(pc$rotation) %>%\n  rownames_to_column(var = \"Variable\") %>%\n  pivot_longer(c(\"PC1\", \"PC2\", \"PC3\"),\n               names_to = \"Component\", \n               values_to = \"Loading\")\n\nggplot(df, aes(x = as.factor(Variable), y = Loading, \n               group = Component, color = Component)) +\n  geom_line() +\n  geom_point() + \n  xlab(\"Variable\")\n\n\n\n\n\nFigure 1.5: The loadings plot shows how the original variables are related to the principal components.\n\n\n\n\n\n\n1.3.3 The number of principal components\nWe now know how to calculate the \\(D\\) principal components for a given \\(D\\)-dimensional dataset, and we’ve seen that the principal components correspond to the directions in which the dataset varies the most. The real power of PCA, and the reason why it is so ubiquitous in data analysis, is that we can now selectively discard principal components that are not informative. By doing this, we obtain a dataset with fewer features, which is hence easier to analyze, and where the discarded features do not contribute too much to the expressivity of the data. This is what makes PCA into a dimensionality reduction method.\nThe question remains what principal components to discard. There are no universally accepted rules for this, but there are a couple of rules of thumb that can help us make an informed choice. Most of these rules take into account the total amount of variance retained by the first \\(K\\) principal components, defined as \\[\n  S_K = \\frac{\\sum_{i=1}^K\\lambda_i}{\\sum_{j=1}^D \\lambda_j},\n\\] Recall that the total amount of variance can be computed directly within R by using the summary command (and look for the “Cumulative Proportion” row):\n\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2     PC3\nStandard deviation     7.2046 3.7433 0.13308\nProportion of Variance 0.7872 0.2125 0.00027\nCumulative Proportion  0.7872 0.9997 1.00000\n\n\nThe number \\(K\\) of principal components can be chosen so that a fixed amount of variance (for example, 95%) is retained. To see this idea in action, let’s apply it to the body fat dataset. The relative amount of variance explained by each principal component can then be calculated as follows:\n\nvar_explained <- pc$sdev^2 / sum(pc$sdev^2)\nvar_explained\n\n[1] 0.787222422 0.212508963 0.000268615\n\n\nand the total amount of variance explained cumulatively by\n\ntotal_var <- cumsum(var_explained)\ntotal_var\n\n[1] 0.7872224 0.9997314 1.0000000\n\n\nNote that these numbers agree with the output of the summary command. For what follows, it will be easier to have access to these quantities as straightforward R vectors.\nWe see that the first two principal components explain 78.7% and 21.3% of the total variance, respectively, and together they explain more than 99.97% of variance in the dataset. It therefore seems reasonable to discard the last principal component, which contributes less than 0.03% of variance.\nFor datasets with many features, a scree plot or elbow plot can be helpful to identify high-variance principal components. In a scree plot, the amounts of variance are plotted in descending order, so that one can identify at a glance the amount of variability contributed by each principal component. Some scree plots also include the total amount of variability, \\(S_K\\), as a function of \\(K\\).\nR can make a pretty basic scree plot for you, via the screeplot command.\n\nscreeplot(pc, main = \"Principal components\", type = \"lines\")\n\n\n\n\nA scree plot made with the base R screeplot command.\n\n\n\n\nWith a bit more work, you can build your own scree plot, which is often preferable if you want to customize the plot, to include for example the cumulative variance, as in figure Figure 1.6.\n\n\nCode\ngg_screeplot <- function(pc, n = length(pc$sdev)) {\n  sdev = pc$sdev[1:n]\n  var_explained <- sdev^2 / sum(sdev^2)\n  total_var <- cumsum(var_explained)\n  df_var <- data.frame(\n    n = seq(1, n), v = var_explained, t = total_var)\n  ggplot(df_var) +\n    geom_line(aes(x = n, y = v, color = \"Per component\")) + \n    geom_point(aes(x = n, y = v, color = \"Per component\")) + \n    geom_line(aes(x = n, y = t, color = \"Cumulative\")) +\n    geom_point(aes(x = n, y = t, color = \"Cumulative\")) +\n    ylim(c(0, 1)) +\n    scale_color_manual(\n      name = \"Explained variance\",\n      breaks = c(\"Per component\", \"Cumulative\"),\n      values = c(\"Per component\" = \"cornflowerblue\",\n                 \"Cumulative\" = \"chocolate\")\n    ) + \n    scale_x_continuous(breaks = n) +\n    xlab(\"Principal component\") +\n    ylab(\"Explained variance (%)\")\n}\n\ngg_screeplot(pc)\n\n\n\n\n\nFigure 1.6: A scree plot for the body fat dataset confirms that the first two principal components explain almost all of the variance in the data.\n\n\n\n\nScree plots are often also referred to as “elbow plots”, since the plot typically (but not always) shows an “elbow” where the explained variance levels off. However, spotting the exact location of the elbow is very subjective, and it is typically more appropriate to take the point where the remaining principal components only contribute some small percentage of variation, for example 5%.\n\n\n1.3.4 Biplots\nThe biplot consists of a loadings plot overlaid on a score plot. By default, it shows the first two principal components as a scatter plot, together with a set of vectors, one for each original variable, showing the contribution of that variable to the first two principal components. Biplots are relatively complex, but it is worth understanding what they encode.\n\nbiplot(pc)\n\n\n\n\nFigure 1.7: The biplot provides information about the transformed data and the loadings.\n\n\n\n\nThe numbers on the plot represent the data points from the original dataset, expressed relative to the first two principal components. This is the part of the biplot that is like a score plot. The red arrows, on the other hand, represent the original variables in the dataset (as shown by the labels attached to them) and are expressed on the top and right-most axis, which show how much each variable contributes to the first two principal components. The red arrows carry the same information as a loadings plot (in a different form), when you consider only the first two principal component.\nAt one glance, we see that triceps.skinfold and thigh.circumference are the most important contributors to the first principal component, and that the second principal component is almost entirely made up by midarm.circumference. This confirms our intuition from the beginning of this chapter, as well as the conclusions that we drew from the loadings plot."
  },
  {
    "objectID": "pca.html#principal-component-regression",
    "href": "pca.html#principal-component-regression",
    "title": "1  Principal component analysis",
    "section": "1.4 Principal component regression",
    "text": "1.4 Principal component regression\nOnce we have performed PCA, we can build a linear model using the principal components that we have chosen to retain. This is known as principal component regression. Let’s see this in action for the bodyfat dataset. We start from a model where we include the first principal component only:\n\npc1 <- pc$x[, \"PC1\"]\noutcome <- bodyfat$bodyfat\n\nmodel_1 <- lm(outcome ~ pc1)\nsummary(model_1)\n\n\nCall:\nlm(formula = outcome ~ pc1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1357 -1.8821  0.2682  1.7107  3.4992 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 20.19500    0.58688  34.411  < 2e-16 ***\npc1          0.61366    0.08358   7.343 8.13e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.625 on 18 degrees of freedom\nMultiple R-squared:  0.7497,    Adjusted R-squared:  0.7358 \nF-statistic: 53.91 on 1 and 18 DF,  p-value: 8.128e-07\n\n\nAdding the second principal component results in a bigger model, but the coefficient of the second principal component is not significant. We therefore do not include it in our final model and we stay with model_1.\n\npc2 <- pc$x[, \"PC2\"]\nmodel_12 <- lm(outcome ~ pc1 + pc2)\nsummary(model_12)\n\n\nCall:\nlm(formula = outcome ~ pc1 + pc2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9876 -1.8822  0.2562  1.3209  4.0285 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 20.19500    0.56604  35.678  < 2e-16 ***\npc1          0.61366    0.08061   7.613 7.12e-07 ***\npc2         -0.23785    0.15514  -1.533    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.531 on 17 degrees of freedom\nMultiple R-squared:  0.7801,    Adjusted R-squared:  0.7542 \nF-statistic: 30.15 on 2 and 17 DF,  p-value: 2.564e-06\n\n\nIn this way, we obtain a simple least squares model, with one predictor and one outcome. Quite a simplification from our original linear model, which had three predictors and suffered from multi-collinearity. This reduction from 3 to 1 predictors may not seem like a big deal, but in the next chapter we will see examples of how PCA can be used to reduce datasets with many hundreds of variables to just 2-3 components, while maintaining the essential information in the dataset.\nIn this section we have built the linear model “by hand”, but in reality you would probably not want to do this, for two reasons:\n\nTesting each predictor at a time when there are many hundreds of candidates to choose from is impractical.\nUsing the forward model building procedure, as we did above, comes with significant multiple-testing issues.\n\nLuckily, there are R packages that will do the PCA, select the optimal number of components, and build the regression model all for us. Here we use the pls package. We tell it to consider all principal components, and to assess the error through cross-validation. The details of this will be discussed in class, but the important line in the output is labeled RMSEP. This shows a measure of error in function of the number of components taken into account. We see that this error is the lowest when only the first principal component is considered.\n\nlibrary(pls)\n\npcr_model <- pcr(bodyfat ~ ., data = bodyfat, validation = \"CV\")\nsummary(pcr_model)\n\nData:   X dimension: 20 3 \n    Y dimension: 20 1\nFit method: svdpc\nNumber of components considered: 3\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps\nCV           5.239    2.612    2.597    2.681\nadjCV        5.239    2.602    2.583    2.656\n\nTRAINING: % variance explained\n         1 comps  2 comps  3 comps\nX          78.72    99.97   100.00\nbodyfat    74.97    78.01    80.14\n\n\nThere are various rules to select the optimal number of components to consider. One such rule is the “1-sigma rule” (Hastie, Tibshirani, and Friedman 2009) which considers the model with the least number of components whose cross-validation error is at most one standard deviation away from the optimal model. In our case, this again results in a model with only one predictor.\n\n\nCode\nselectNcomp(pcr_model, method = \"onesigma\", plot = TRUE)\n\n\n\n\n\n[1] 1\n\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Vol. 2. Information Science and Statistics. Springer, New York.\n\n\nHastie, T., R. Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer."
  },
  {
    "objectID": "pca-applications.html",
    "href": "pca-applications.html",
    "title": "2  Applications of principal component analysis",
    "section": "",
    "text": "Principal component analysis is often a necessary first step when there are a large number of independent variables that need to be analyze simultaneously. Many devices in a modern lab produce this kind of high-dimensional data: for example, a reading for a single sample obtained via gas chromatography-mass spectrometry (GC-MS) or hyperspectral imagining (HSI) is a vector with 100s of entries, and with the number of samples often running in the 100s as well, we need a technique like PCA to find the needle in the haystack.\nIn this chapter we consider a number of real-world examples, from the life sciences and beyond, where PCA and dimensionality reduction prove to be essential.\nThese coures notes are made available under the Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "pca-applications.html#adulteration-of-olive-oil",
    "href": "pca-applications.html#adulteration-of-olive-oil",
    "title": "2  Applications of principal component analysis",
    "section": "2.1 Adulteration of olive oil",
    "text": "2.1 Adulteration of olive oil\nThis case study is a simplified version of an analysis done by the group of Prof. Van Haute of the Centre for Food Chemistry and Technology at GUGC to determine the adulteration of extra-virgin olive oil through the use of hyperspectral imaging data. The full analysis can be found in (Malavi et al. 2023). My thanks go to Prof. Van Haute for making the data available and for in-depth discussions regarding the analysis.\n\n2.1.1 Problem statement\nExtra-virgin olive oil (EVOO) is a type of oil that is made by cold-pressing the fruit of the olive tree without the use of chemicals or heat. It is considered the highest quality and most flavorful type of olive oil and is widely used in cooking and as a dressing for salads and other dishes. Extra-virgin olive is packed with antioxidants and healthy fats, making it a popular choice among health-conscious consumers. Due to its high quality and health benefits, extra-virgin olive oil is often more expensive than other types of olive oil.\nExtra-virgin olive oil is sometimes adulterated, either accidentally, or deliberately, by the addition of other, cheaper vegetable oils. This is misleading to the customer and can pose health risks, for example through the introduction of allergens. As a result, manufacturers and food safety agencies have an interest in determining whether a given EVOO sample has been adulterated, and if so, to what degree.\nOne way to determine the chemical composition of an oil sample is through hyperspectral imaging (HSI). A hyperspectral imaging system will shine infrared light onto the sample and measure the reflection off the sample at different wavelengths. We will not go into the details of how this signal is acquired, but what is important is that the system outputs for each sample a so-called spectral curve describing the average reflectance at different wavelengths. By inspecting the spectral curve, we can establish whether the sample absorbs light of a specific wavelength, and this can point towards the presence of particular chemical structures that characterize the sample. An example of a spectral curve is shown in figure Figure 2.1.\nIn our case study we want to determine whether hyperspectral imaging can be used to detect the adulteration of extra-virgin olive oil. More precisely, we have the following research questions:\n\nCan hyperspectral imaging be used to detect whether olive oil has been adulterated with other kinds of oils?\nIf so, can the amount of adulteration be quantified (e.g. as a percentage)?\n\nTo investigate these research questions, Malavi et al. (2023) acquired readings from 13 different kinds of unadulterated EVOO, together with readings from 42 adulterated mixtures. Each adulterated mixture was prepared by taking olive oil and adding one of 6 different vegetable oils at 7 different percentages (ranging from 1% to 20% adulterant). Each sample was prepared and imagined in triplicate, resulting in 183 spectra. Each spectrum is a vector of length 224, describing the reflectance at 224 equally distributed wavelengths from 700 to 1800 nm.\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 2.1: Hyperspectral imaging system (left) and typical output spectra (right). Figure source: Malavi et al. (2023).\n\n\nBelow, we read in the dataset and we print out the first 6 columns of a random sample of 10 rows (the full dataset is too large to display in its entirety). Note that the dataset has 228 columns: 4 of these are metadata variables described below and the remaining 224 columns describe the spectrum for each sample. The metadata variables are:\n\nSample ID/Wavelength: The name of the oil or mixture, and the mixture ratio (if applicable)\nSample: A unique integer identifying each sample. Not used in the subsequent analysis.\nClassification: Whether the sample is primarily olive oil or not.\n% Adulteration: The percentage of food oil added to the mixture. For pure EVOO this is 0, while for pure food oil it is 100%. For the other mixtures, it is one of 1%, 2%, 4%, 8%, 12%, 16%, or 20%.\n\n\n\nCode\nlibrary(readxl)\n\n# Read in the data and add the 'mixture' column described above.\n oils <- read_excel(\"./datasets/02-pca/HSI.xlsx\")\n\n# Names of the spectra.\ncols <- colnames(oils)\nspectra <- cols[5:length(cols)]\n\n# \"Long\" form of the dataframe, used for plotting.\noils_long <- oils %>%\n  pivot_longer(cols = spectra, \n               names_to = \"wavelength\", \n               values_to = \"intensity\") %>%\n  mutate(wavelength = as.numeric(wavelength))\n\noils[sample(1:nrow(oils), 10), 1:6]\n\n\n# A tibble: 10 × 6\n   `Sample ID/Wavelength`       Sample Classification % Adulte…¹ 938.9…² 942.4…³\n   <chr>                         <dbl> <chr>               <dbl>   <dbl>   <dbl>\n 1 EVOO flavored with Chilli        28 Olive                   0   0.638   0.643\n 2 Olive Oil/Corn Oil 99/1          80 Olive                   1   0.649   0.655\n 3 Olive Oil/Sunflower Oil 96/4    150 Olive                   4   0.659   0.664\n 4 Olive Oil/Soybean Oil 99/1      101 Olive                   1   0.609   0.613\n 5 Olive Oil/Soybean Oil 92/8      111 Olive                   8   0.632   0.636\n 6 Olive Oil/Canola Oil 84/16      137 Olive                  16   0.652   0.657\n 7 Olive Oil/Canola Oil 88/12      133 Olive                  12   0.655   0.660\n 8 Olive Oil/Sesame Oil 98/2       166 Olive                   2   0.652   0.657\n 9 Olive Oil/Sunflower Oil 99/1    144 Olive                   1   0.652   0.657\n10 Olive Oil/Canola Oil 92/8       132 Olive                   8   0.654   0.660\n# … with abbreviated variable names ¹​`% Adulteration`, ²​`938.94000200000005`,\n#   ³​`942.45001200000002`\n\n\n\n\n2.1.2 Inspecting the spectral plots\nAs a first step in our analysis, let’s compare the typical spectrum of EVOO with that of the other 6 vegetable oils. We see that overall the spectra are quite similar (they are all oils, after all) but that there are small differences between the different kinds of oil. On the other hand, if we are given a new, unlabeled spectrum, it would be quite difficult to “guess” just by looking what type of oil it is. This is where dimensionality reduction will help us!\n\n\nCode\n# Keep only EVOO and pure vegetable oil, and compute mean reflectivity across \n# the 3 replicates\nsummarized_oils_long <- oils_long %>%\n  filter(`% Adulteration` == 0 | `% Adulteration` == 100) %>%\n  mutate(type = if_else(Classification == \"Olive\", \"EVOO\", `Sample ID/Wavelength`)) %>%\n  group_by(type, wavelength) %>% summarise(mean_intensity = mean(intensity))\n\nggplot(\n  summarized_oils_long,\n  aes(x = wavelength, y = mean_intensity, color = type)) +\n  geom_line() +\n  labs(x = \"Wavelength (nm)\", y = \"Mean reflectance\", color = \"Type of oil\")\n\n\n\n\n\nTypical spectra of EVOO and different types of vegetable oils. Note the similarity with figure Figure 2.1 (b).\n\n\n\n\n\n\n2.1.3 Computing the principal components\nWe are now in a position to compute the principal components, and to figure out what they tell us about olive and other oils. Two issues to keep in mind:\n\nOur dataset contains both metadata and spectral data: we want to make sure to compute the principal components only for the columns containing spectral data!\nFor spectral data, we definitely do not scale the different variables!\n\n\npca_oils <- oils %>%\n  select(all_of(spectra)) %>%\n  prcomp(scale = FALSE)\n\nThe first two principal components explain more than 90% of the variance in the data, as shown below.\n\n\nCode\npca_oils %>%\n  broom::tidy(matrix = \"eigenvalues\") %>%\n  head(n = 9) %>%\n  ggplot(aes(PC, percent)) +\n  geom_col(fill = \"#56B4E9\", alpha = 0.8) +\n  scale_x_continuous(breaks = 1:9) +\n  scale_y_continuous(\n    labels = scales::percent_format(),\n    expand = expansion(mult = c(0, 0.01))\n  ) +\n  labs(y = \"Percentage of variance explained\")\n\n\n\n\n\nPercentage of variance explained by the first 10 principal components. The first two principal components explain 94% of the variance in the data.\n\n\n\n\nThe loadings for the first two principal components tell us something about what the spectra look like overall, and what variations between the individual spectra look like. This is of course an extremely coarse perspective, given that we only consider two principal components.\n\n\nCode\nloadings <- as.data.frame(pca_oils$rotation)\nloadings$wavelength <- as.numeric(rownames(loadings))\nrownames(loadings) <- 1:nrow(loadings)\n\nloadings %>%\n  select(wavelength, PC1, PC2) %>%\n  pivot_longer(cols = c(PC1, PC2)) %>%\n  ggplot() +\n    geom_line(aes(x = wavelength, y = value, color = name), linewidth = 1.0) +\n  labs(x = \"Wavelength\", y = \"\", color = \"Component\")\n\n\n\n\n\nLoadings for the first two principal components.\n\n\n\n\nWe started this section with the observation that there are minute differences between the spectra of different oils, but we admitted that it would be quite difficult to tell two spectra apart just by eye. We are now in a position where we can solve this issue: by considering only the first few principal components we get a reduced number of variables that (hopefully) can be used to tell the spectra apart.\nBelow we have a scatterplot of the first two principal components. The EVOO samples are round dots, and the adulterated oils are triangles colored by the percentage of adulteration. The EVOO samples are clearly distinct from the adulterated oils (they are separated by the second principal component). This answers our first research question with a “yes”: we can clearly tell pure and adulterated oils apart.\n\n\nCode\npdata <- pca_oils %>%\n  broom::augment(oils) %>%\n  filter(`% Adulteration` < 100) %>%\n  mutate(adulterated = `% Adulteration` > 0)\n\npct_var_explained <- 100*pca_oils$sdev^2/sum(pca_oils$sdev^2)\nxlabel <- paste0(\"PC 1 (\", round(pct_var_explained[[1]], 2), \"% var. explained)\")\nylabel <- paste0(\"PC 2 (\", round(pct_var_explained[[2]], 2), \"% var. explained)\")\n\nggplot(\n  pdata,\n  aes(.fittedPC1, .fittedPC2,\n      color = `% Adulteration`, shape = adulterated)) +\n  geom_point(size = 2) +\n  labs(x = xlabel, y = ylabel, shape = \"Type of oil\") +\n  scale_shape_discrete(labels = c(\"Pure\", \"Adulterated\"))\n\n\n\n\n\nFigure 2.2: Principal component plot for the EVOO and adulterated oil samples. Pure vegetable oil samples are not shown.\n\n\n\n\n\n\n2.1.4 Predicting the percentage of adulteration\nNow that we are able to tell EVOO and adulterated samples apart, it is time to consider our second research question: given an adulterated oil sample, can we predict the percentage of vegetable oil that was added? Looking at Figure 2.2, it looks like we’ll need more than two principal components: we see no clear pattern in the percentage adulteration that we or a linear model could exploit.\nBefore building our model, we need to do some data preparation. We select from our dataset only those rows that contain adulterated oils, and we split the resulting dataset into a training dataset (containing 80% of the data) and a test dataset (containing the remaining 20%). The idea is that we set apart the test dataset and build our model using only the training dataset. The test dataset is used to evaluate how well the model performs on data that it has never seen before. Using the full dataset to build the model and evaluate it would result in an overly optimistic estimate of the predictive capabilities of our model.\n\n\nCode\n# for the adulterated oils, predict the percentage of adulteration\nadulterated <- oils %>%\n  filter(`% Adulteration` > 0, `% Adulteration` <= 20) %>%\n  select(-`Sample ID/Wavelength`,\n         -Sample,\n         -Classification)\n\n# Set aside some test data\ntrain_no <- round(0.8 * nrow(adulterated))\ntrain_idxs <- sample(1:nrow(adulterated), train_no)\nadulterated_train <- adulterated[train_idxs, ]\nadulterated_test <- adulterated[-train_idxs, ]\n\nmessage(\"Number of rows in the training dataset: \", nrow(adulterated_train))\n\n\nNumber of rows in the training dataset: 101\n\n\nCode\nmessage(\"Number of rows in the test dataset: \", nrow(adulterated_test))\n\n\nNumber of rows in the test dataset: 25\n\n\nWe could build a linear model with more than two components by hand, just like we did for the bodyfat dataset in the previous chapter. However, it is easier to let R do the work for us, and we will use the the pls package for this. This package has a number of advantages: you can use the same formula syntax that you know from the lm command, and the package comes with a number of different regression models. Here we build a principal component regression model and a partial least squares regression model.\n\nlibrary(pls)\n\n# Principal component regression\npcr_model <- pcr(\n  `% Adulteration` ~ ., data = adulterated_train, scale = FALSE, validation = \"CV\", ncomp = 10\n)\n\n# Partial least squares regression\npls_model <- plsr(\n  `% Adulteration` ~ ., data = adulterated_train, scale = FALSE, validation = \"CV\", ncomp = 10\n)\n\nThe argument validation = \"CV\" will cause the model to select by itself the appropriate number of components to use, up to a maximum of 10 components (this is controlled by the ncomp = 10 argument). To do this it will use the so-called cross-validation (CV) strategy: it will repeatedly split the data into a training and a validation set (not to be confused with the training and test set that we created ourselves above) and use the performance on the validation set to select the appropriate number of components.\nNow that we have two models, we can ask them to make predictions on the held-out test data, and we can compare the predictions made between both models. In the prediction plot below we see in general good agreement between the actual percentage of adulteration (on the \\(x\\)-axis) and the predicted percentage (on the \\(y\\)-axis). Both models perform comparably well, at least as far as can be judged from the plot.\n\npcr_pred <- predict(pcr_model, adulterated_test, ncomp = 10)\npls_pred <- predict(pls_model, adulterated_test, ncomp = 10)\n\n\n\nCode\n# Plot both results together\ndf <- data.frame(\n  measured = adulterated_test$`% Adulteration`,\n  PLS = unlist(as.list(pls_pred)),\n  PCR = unlist(as.list(pcr_pred))\n) %>% pivot_longer(cols = c(\"PLS\", \"PCR\"))\n\njitter_x <- position_jitter(w = 0.15, h = 0)\nggplot(df) +\n  geom_abline(alpha = 0.3) +\n  geom_point(aes(x = measured, y = value, color = name),\n             alpha = 1.0, position = jitter_x) +\n  labs(color = \"Method\", x = \"Measured\", y = \"Predicted\")\n\n\n\n\n\n\n\n2.1.5 A cautionary note\nIn our model building procedure we have glossed over a few critical steps that may cause doubt on the validity of the model or the conclusions that we can draw from our analysis.\n\nChiefly among these is the splitting into training and test data. We did that correctly to train our model, but everything that came before it (including the exploratory data analysis) used the entire data set. To build a model that is not overly optimistic, you should set aside some data to serve as test data at the beginning of your analysis, and then use only the training data to determine the appropriate number principal components, to classify oils into adulterated and pure, and so on. Your model should never “see” the test data, until the very end.\nWe have trained and evaluated our model only on adulterated oils. In reality, however, we will present the model with unknown oil samples, which may or may not be adulterated, and the performance on such samples is an open question. A better model could be built by including EVOO samples in our dataset, or by first classifying oils into adulterated/pure. Each approach comes with a number of complications that would lead us too far.\nOur model willfully ignores a number of important variables in the dataset. For example, we don’t take into account the kind of vegetable oil that was used to adulterate a sample. By distinguishing the type of adulterant, we could presumably build a better model, but this too would make the analysis significantly more complex.\n\nA more complete model that addresses all of these concerns (and more) can be found in (Malavi et al. 2023)."
  },
  {
    "objectID": "pca-applications.html#sec-eigenfaces",
    "href": "pca-applications.html#sec-eigenfaces",
    "title": "2  Applications of principal component analysis",
    "section": "2.2 Eigenfaces",
    "text": "2.2 Eigenfaces\nOur last example is not life sciences based, but serves as an illustration to show that PCA is a powerful technique in data analysis, which can be used to reduce the number of degrees of freedom in a large dataset.\nWe use the Olivetti dataset of human faces, which contains 400 frontal photographs of human faces. Each face is a grayscale image of 64 by 64 pixels, where the intensity of each pixel is a value between 0 (completely black) to 255 (completely white). Each image can be represented as a \\(64 \\times 64\\) matrix, but it will be more convenient to take the columns of this matrix and lay them out one after the other to obtain a vector with \\(64 \\times 64 = 4096\\) entries, as in Figure 2.3.\n\n\n\nFigure 2.3: An image that is \\(N\\) pixels high and \\(M\\) pixels wide can be viewed as a matrix with \\(N\\) rows and \\(M\\) columns, or as a vector with \\(N \\times M\\) elements. Here, \\(N\\) and \\(M\\) are both equal to 3.\n\n\nFirst, we load the dataset. Note that the dataset comes as a data matrix with 4096 rows and 400 columns.\n\nlibrary(loon.data)\ndata(faces)\ndim(faces)\n\n[1] 4096  400\n\n\nEach column in the data matrix represents a face, laid out as a column vector with 4096 as in Figure 2.3. We can assemble these vectors back into images and visualize them. This requires some R commands that we haven’t covered; you don’t have to understand what this code does.\n\n\nCode\nshow_image <- function(imgdata, title = NULL) {\n  m <- matrix(imgdata, nrow = 64, ncol = 64, byrow = F)\n  m <- t(apply(m, 2, rev))\n  image(m, axes = FALSE, col=grey(seq(0,1,length=256)),\n        main = title)\n}\n\npar(mfrow=c(2, 4), mar=c(1, 1, 1, 1))\nfor (i in 1:8) {\n  show_image(faces[,10*i], paste0(\"Face #\", 10*i))\n}\n\n\n\n\n\nFigure 2.4: Six faces from the Olivetti dataset.\n\n\n\n\nDoing a principal component analysis is a simple matter of running prcomp. Despite the size of the dataset, this component should not take more than a second to run.\n\npc_olivetti <- prcomp(faces)\n\nNote that there are 400 principal components in this dataset. We can visualize their relative importance via a scree plot, which we limit to the first 50 components for clarity, since the remaining 350 components contribute almost no variance. This indicates that we can probably discard most of the principal components without losing much of the expressivity of our dataset. We will see further down that this is indeed the case!\n\n\nCode\ngg_screeplot(pc_olivetti, n = 50)\n\n\n\n\n\nOne of the advantages of the faces dataset is that the principal components can be represented graphically, and that we can reason about them. Figure 2.5 shows the first 8 principal components, represented as images. How should we interpret these images? Each principal component represents a particular pattern in the dataset of all faces: the first principal component, for example, captures the overall structure of a human face, while the second represents the illumination from right to left. Probably there were some photos in the dataset that were illuminated from the left or the right. Principal component three does the same for the top-down illumination, and principal components four through eight capture particular patterns involving the eyes or the eyebrows. By selectively “mixing” all 400 principal components, we can recreate any face in the dataset.\n\n\nCode\nnormalize <- function(x) {\n  # ensure that PC components are between 0 and 255,\n  # for visualization\n  255 * (x - min(x)) / (max(x) - min(x))\n}\npar(mfrow=c(2, 4), mar=c(1, 1, 1, 1))\nfor (i in 1:8) {\n  show_image(normalize(pc_olivetti$x[,i]), \n             paste0(\"PC \", i))\n}\n\n\n\n\n\nFigure 2.5: The first 8 principal components of the Olivetti dataset represent particularly expressive patterns in the dataset.\n\n\n\n\nTo finish, let’s also investigate how well PCA performs as a data reduction method. By retaining only a limited number of principal components, we can build “reduced” versions of the images that involve only a number of principal components. Figure 2.6 shows two original faces from the dataset (left), together with compressed versions involving the first 10, 40, and 80 most significant principal components. The version that uses only 10 components is quite generic and it is difficult even to distinguish the male and female face. The version with 80 components, on the other hand, is very close to the original.\n\n\nCode\nproject_data <- function(pc, n_retain) {\n  t(t(pc$x[,1:n_retain] %*% t(pc$rotation)[1:n_retain,]) + pc$center)\n}\n\npar(mfrow=c(2, 4), mar=c(1, 1, 1, 1))\nshow_image(faces[,70], \"Original\")\nshow_image(project_data(pc_olivetti, 10)[,70], \"10 PCs\")\nshow_image(project_data(pc_olivetti, 40)[,70], \"40 PCs\")\nshow_image(project_data(pc_olivetti, 80)[,70], \"80 PCs\")\n\nshow_image(faces[,80], \"Original\")\nshow_image(project_data(pc_olivetti, 10)[,80], \"10 PCs\")\nshow_image(project_data(pc_olivetti, 40)[,80], \"40 PCs\")\nshow_image(project_data(pc_olivetti, 80)[,80], \"80 PCs\")\n\n\n\n\n\nFigure 2.6: Original images (left), and 3 PCA-reduced images with increasing numbers of principal components.\n\n\n\n\nIt is worth realizing the amount of data compression realized by using PCA. The original images had 4096 degrees of freedom, whereas the rightmost versions in Figure 2.6 are described by 80 loadings, more than a 50-fold reduction in degrees of freedom! Clearly there are some visual artifacts that appear in the compressed versions, but the faces are clearly distinguishable, and it seems very reasonable at this point that a machine learning algorithm (for example, to classify the faces, or to do segmentation) could take these compressed images as input and still perform well.\n\n\n\n\nMalavi, Derick, Amin Nikkhah, Katleen Raes, and Sam Van Haute. 2023. “Hyperspectral Imaging and Chemometrics for Authentication of Extra Virgin Olive Oil: A Comparative Approach with FTIR, UV-VIS, Raman, and GC-MS.” Foods 12 (3): 429. https://doi.org/10.3390/foods12030429."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "These coures notes are made available under the Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Appendix A — Datasets",
    "section": "",
    "text": "These coures notes are made available under the Creative Commons BY-NC-SA 4.0 license."
  }
]