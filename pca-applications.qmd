# Applications of principal component analysis

```{r}
#| include: false
set.seed(1234)

library(ggplot2)
library(GGally)
library(gridExtra)
library(tidyverse)

# From previous chapter -- move this to utility module?
gg_screeplot <- function(pc, n = length(pc$sdev)) {
  sdev = pc$sdev[1:n]
  var_explained <- sdev^2 / sum(sdev^2)
  total_var <- cumsum(var_explained)
  df_var <- data.frame(
    n = seq(1, n), v = var_explained, t = total_var)
  ggplot(df_var) +
    geom_line(aes(x = n, y = v, color = "Per component")) + 
    geom_point(aes(x = n, y = v, color = "Per component")) + 
    geom_line(aes(x = n, y = t, color = "Cumulative")) +
    geom_point(aes(x = n, y = t, color = "Cumulative")) +
    ylim(c(0, 1)) +
    scale_color_manual(
      name = "Explained variance",
      breaks = c("Per component", "Cumulative"),
      values = c("Per component" = "cornflowerblue",
                 "Cumulative" = "chocolate")
    ) + 
    scale_x_continuous(breaks = n) +
    xlab("Principal component") +
    ylab("Explained variance (%)")
}

```

Principal component analysis is often a necessary first step when there are a large number of independent variables that need to be  analyze simultaneously. Many devices in a modern lab produce this kind of high-dimensional data: for example, a reading for a single sample obtained via gas chromatography-mass spectrometry (GC-MS) or hyperspectral imagining (HSI) is a vector with 100s of entries, and with the number of samples often running in the 100s as well, we need a technique like PCA to find the needle in the haystack.

In this chapter we consider a number of real-world examples, from the life sciences and beyond, where PCA and dimensionality reduction prove to be essential.

::: {.callout-warning}
Given that the examples in this chapter deal with real-world data, which is often large and messy, the R code is at times a bit more complex than in the previous chapter. The underlying principles remain the same, however.
:::

## Adulteration of olive oil

This case study is a simplified version of an analysis done by the group of Prof. Van Haute of the Centre for Food Chemistry and Technology at GUGC to determine the adulteration of extra-virgin olive oil through the use of hyperspectral imaging data. The full analysis can be found in [@2023-malavi-HyperspectralImagingChemometrics]. My thanks go to Prof. Van Haute for making the data available and for in-depth discussions regarding the analysis.

### Problem statement

Extra-virgin olive oil (EVOO) is a type of oil that is made by cold-pressing the fruit of the olive tree without the use of chemicals or heat. It is considered the highest quality and most flavorful type of olive oil and is widely used in cooking and as a dressing for salads and other dishes. Extra-virgin olive is  packed with antioxidants and healthy fats, making it a popular choice among health-conscious consumers. Due to its high quality and health benefits, extra-virgin olive oil is often more expensive than other types of olive oil. 

Extra-virgin olive oil is sometimes adulterated, either accidentally, or deliberately, by the addition of other, cheaper vegetable oils. This is misleading to the customer and can pose health risks, for example through the introduction of allergens. As a result, manufacturers and food safety agencies have an interest in determining whether a given EVOO sample has been adulterated, and if so, to what degree.

One way to determine the chemical composition of an oil sample is through hyperspectral imaging (HSI). A hyperspectral imaging system will shine infrared light onto the sample and measure the reflection off the sample at different wavelengths. We will not go into the details of how this signal is acquired, but what is important is that the system outputs for each sample a so-called *spectral curve* describing the average reflectance at different wavelengths. By inspecting the spectral curve, we can establish whether the sample absorbs light of a specific wavelength, and this can point towards the presence of particular chemical structures that characterize the sample. An example of a spectral curve is shown in figure @fig-hsi.

In our case study we want to determine whether hyperspectral imaging can be used to detect the adulteration of extra-virgin olive oil. More precisely, we have the following research questions:

1. *Can hyperspectral imaging be used to detect whether olive oil has been adulterated with other kinds of oils?*
1. *If so, can the amount of adulteration be quantified (e.g. as a percentage)?*

To investigate these research questions, @2023-malavi-HyperspectralImagingChemometrics acquired readings from 13 different kinds of unadulterated EVOO, together with readings from 42 adulterated mixtures. Each adulterated mixture was prepared by taking olive oil and adding one of 6 different vegetable oils to it, at 7 different percentages (ranging from 1% to 20% adulterant). Each sample was prepared and imagined in triplicate, resulting in 183 spectra. Each spectra is a vector of length 224, describing the reflectance at equally distributed wavelengths from 700 to 1800 nm.

::: {#fig-hsi layout-ncol=2 align="center" layout-valign="center"}

![](./images/02-PCA/hyperspectral.png){#fig-hyperspectral-system}

![](./images/02-PCA/hsi-spectra.png){#fig-typical-spectra}

Hyperspectral imaging system (left) and typical output spectra (right). Figure source: @2023-malavi-HyperspectralImagingChemometrics. 
:::

Below, we read in the dataset and we print out a random sample of 10 rows from it. Note that the dataset has 228 columns: 4 of these are metadata variables described below and the remaining 224 columns describe the spectrum for each sample. The metadata variables are:

- `Sample ID/Wavelength`: The name of the oil or mixture, and the mixture ratio (if applicable)
- `Sample`: A unique integer identifying each sample. Not used in the subsequent analysis.
- `Classification`: Whether the sample is primarily olive oil or not.
- `% Adulteration`: The percentage of food oil added to the mixture. For pure EVOO this is 0, while for pure food oil it is 100%. For the other mixtures, it is one of 1%, 2%, 4%, 8%, 12%, 16%, or 20%.

```{r}
#| warning: false

library(readxl)

# Split off the percentage adulterant from the mixture name.
# create_mixture_name <- function(fullname) {
#   if (!grepl('/', fullname, fixed = TRUE)) {
#     return(fullname)
#   }
#   items <- strsplit(fullname, ' +')[[1]]
#   paste(items[-length(items)], collapse = " ")
# }

# Read in the data and add the 'mixture' column described above.
 oils <- read_excel("./datasets/02-pca/HSI.xlsx")# %>%
  # mutate(mixture = map_chr(`Sample ID/Wavelength`, create_mixture))

# Names of the spectra.
cols <- colnames(oils)
spectra <- cols[5:length(cols)]

# "Long" form of the dataframe, used for plotting.
oils_long <- oils %>%
  pivot_longer(cols = spectra, 
               names_to = "wavelength", 
               values_to = "intensity") %>%
  mutate(wavelength = as.numeric(wavelength))

rmarkdown::paged_table(oils[sample(1:nrow(oils), 10),])

```

### Inspecting the spectral plots

As a first step in our analysis, let's compare the typical spectrum of EVOO with that of the other 6 vegetable oils. We see that overall the spectra are quite similar (they are all oils, after all) but that there are small differences between the different kinds of oil. On the other hand, if we are given a new, unlabeled spectrum, it would be quite difficult to "guess" just by looking what type of oil it is. This is where dimensionality reduction will help us!

```{r}
#| warning: false
#| fig-cap: Typical spectra of EVOO and different types of vegetable oils. Note the similarity with figure @fig-typical-spectra.

# Keep only EVOO and pure vegetable oil, and compute mean reflectivity across 
# the 3 replicates
summarized_oils_long <- oils_long %>%
  filter(`% Adulteration` == 0 | `% Adulteration` == 100) %>%
  mutate(type = if_else(Classification == "Olive", "EVOO", `Sample ID/Wavelength`)) %>%
  group_by(type, wavelength) %>% summarise(mean_intensity = mean(intensity))

ggplot(
  summarized_oils_long,
  aes(x = wavelength, y = mean_intensity, color = type)) +
  geom_line() +
  labs(x = "Wavelength (nm)", y = "Mean reflectance", color = "Type of oil")


```


### XYZ

Compute principal components
```{r}
pca_oils <- oils %>%
  select(all_of(spectra)) %>%
  prcomp(scale = FALSE)

```

Percentage of variance explained
```{r}
pca_oils %>%
  broom::tidy(matrix = "eigenvalues") %>%
  head(n = 9) %>%
  ggplot(aes(PC, percent)) +
  geom_col(fill = "#56B4E9", alpha = 0.8) +
  scale_x_continuous(breaks = 1:9) +
  scale_y_continuous(
    labels = scales::percent_format(),
    expand = expansion(mult = c(0, 0.01))
  ) +
  labs(y = "Percentage of variance explained")
```




Plot of the first few loadings vectors
```{r}
loadings <- as.data.frame(pca_oils$rotation)
loadings$wavelength <- as.numeric(rownames(loadings))
rownames(loadings) <- 1:nrow(loadings)

loadings %>%
  select(wavelength, PC1, PC2) %>%
  pivot_longer(cols = c(PC1, PC2)) %>%
  ggplot() +
    geom_line(aes(x = wavelength, y = value, color = name), linewidth = 1.0) +
  labs(x = "Wavelength", y = "", color = "Component")

```

Plot first two PCs
```{r}
pdata <- pca_oils %>%
  broom::augment(oils) %>%
  filter(`% Adulteration` < 100) %>%
  mutate(adulterated = `% Adulteration` > 0)

pct_var_explained <- 100*pca_oils$sdev^2/sum(pca_oils$sdev^2)
xlabel <- paste0("PC 1 (", round(pct_var_explained[[1]], 2), "% var. explained)")
ylabel <- paste0("PC 2 (", round(pct_var_explained[[2]], 2), "% var. explained)")

ggplot(
  pdata,
  aes(.fittedPC1, .fittedPC2,
      color = `% Adulteration`, shape = adulterated)) +
  geom_point(size = 2) +
  labs(x = xlabel, y = ylabel, shape = "Type of oil") +
  scale_shape_discrete(labels = c("Pure", "Adulterated"))

```

```{r}
# for the adulterated oils, predict the percentage of adulteration
adulterated <- oils %>%
  filter(`% Adulteration` > 0, `% Adulteration` <= 20) %>%
  select(-`Sample ID/Wavelength`,
         -Sample,
         -Classification)

# Set aside some test data
train_no <- round(0.8 * nrow(adulterated))
train_idxs <- sample(1:nrow(adulterated), train_no)
adulterated_train <- adulterated[train_idxs, ]
adulterated_test <- adulterated[-train_idxs, ]

```

```{r}
library(pls)

# Run a PCR analysis first
pcr_model <- pcr(
  `% Adulteration` ~ .,
  data = adulterated_train,
  scale = FALSE, validation = "CV"
)
# Do a PLS secondly

pls_model <- plsr(
  `% Adulteration` ~ .,
  data = adulterated_train,
  scale = FALSE, validation = "CV"
)

```

```{r}
ncomp <- 5
pcr_pred <- predict(pcr_model, adulterated_test, ncomp = ncomp)
pls_pred <- predict(pls_model, adulterated_test, ncomp = ncomp)
```

```{r}
# Plot both results together
df <- data.frame(
  measured = adulterated_test$`% Adulteration`,
  PLS = unlist(as.list(pls_pred)),
  PCR = unlist(as.list(pcr_pred))
) %>% pivot_longer(cols = c("PLS", "PCR"))

jitter_x <- position_jitter(w = 0.15, h = 0)
ggplot(df) +
  geom_abline(alpha = 0.3) +
  geom_point(aes(x = measured, y = value, color = name),
             alpha = 1.0, position = jitter_x) +
  labs(color = "Method", x = "Measured", y = "Predicted")

```

```{r}
# Evaluate RMSE as a function of number of components on test dataset

rmse <- function(ncomp, model) {
  pred <- predict(model, adulterated_test, ncomp = ncomp)
  mean((pred - adulterated_test$`% Adulteration`)^2)^0.5
}

ncomps <- seq(3, 15)
rmse_data <- data.frame(
  ncomps = ncomps,
  PCR = map_dbl(ncomps, rmse, model = pcr_model),
  PLS = map_dbl(ncomps, rmse, model = pls_model)
) %>% pivot_longer(cols = c("PCR", "PLS"))

ggplot(rmse_data) +
  geom_line(aes(x = ncomps, y = value, color = name)) +
  labs(color = "Method", x = "Number of components", y = "RMSE")


```


## Eigenfaces {#sec-eigenfaces}

Our last example is not life sciences based, but serves as an illustration to show that PCA is a powerful technique in data analysis, which can be used to reduce the number of degrees of freedom in a large dataset.

We use the Olivetti dataset of human faces, which contains 400 frontal photographs of human faces. Each face is a grayscale image of 64 by 64 pixels, where the intensity of each pixel is a value between 0 (completely black) to 255 (completely white). Each image can be represented as a $64 \times 64$ matrix, but it will be more convenient to take the columns of this matrix and lay them out one after the other to obtain a vector with $64 \times 64 = 4096$ entries, as in @fig-image-unroll.

![An image that is $N$ pixels high and $M$ pixels wide can be viewed as a matrix with $N$ rows and $M$ columns, or as a vector with $N \times M$ elements. Here, $N$ and $M$ are both equal to 3.](images/02-PCA/image-unroll){#fig-image-unroll}

First, we load the dataset. Note that the dataset comes as a data matrix with 4096 rows and 400 columns.

```{r}
#| echo: true
#| code-fold: false
library(loon.data)
data(faces)
dim(faces)
```

Each column in the data matrix represents a face, laid out as a column vector with 4096 as in @fig-image-unroll. We can assemble these vectors back into images and visualize them. This requires some R commands that we haven't covered; you don't have to understand what this code does.

```{r}
#| fig-cap: Six faces from the Olivetti dataset.
#| label: fig-olivetti-faces
show_image <- function(imgdata, title = NULL) {
  m <- matrix(imgdata, nrow = 64, ncol = 64, byrow = F)
  m <- t(apply(m, 2, rev))
  image(m, axes = FALSE, col=grey(seq(0,1,length=256)),
        main = title)
}

par(mfrow=c(2, 4), mar=c(1, 1, 1, 1))
for (i in 1:8) {
  show_image(faces[,10*i], paste0("Face #", 10*i))
}

```

Doing a principal component analysis is a simple matter of running `prcomp`. Despite the size of the dataset, this component should not take more than a second to run.

```{r}
#| echo: true
#| code-fold: false
pc_olivetti <- prcomp(faces)
```

Note that there are 400 principal components in this dataset. We can visualize their relative importance via a scree plot, which we limit to the first 50 components for clarity, since the remaining 350 components contribute almost no variance. This indicates that we can probably discard most of the principal components without losing much of the expressivity of our dataset. We will see further down that this is indeed the case!

```{r}
#| warning: false
gg_screeplot(pc_olivetti, n = 50)
```

One of the advantages of the faces dataset is that the principal components can be represented graphically, and that we can reason about them. @fig-olivetti-pc shows the first 8 principal components, represented as images. How should we interpret these images? Each principal component represents a particular *pattern* in the dataset of all faces: the first principal component, for example, captures the overall structure of a human face, while the second represents the illumination from right to left. Probably there were some photos in the dataset that were illuminated from the left or the right. Principal component three does the same for the top-down illumination, and principal components four through eight capture particular patterns involving the eyes or the eyebrows. *By selectively "mixing" all 400 principal components, we can recreate any face in the dataset.*

```{r}
#| fig-cap: The first 8 principal components of the Olivetti dataset represent particularly expressive patterns in the dataset.
#| label: fig-olivetti-pc
normalize <- function(x) {
  # ensure that PC components are between 0 and 255,
  # for visualization
  255 * (x - min(x)) / (max(x) - min(x))
}
par(mfrow=c(2, 4), mar=c(1, 1, 1, 1))
for (i in 1:8) {
  show_image(normalize(pc_olivetti$x[,i]), 
             paste0("PC ", i))
}

```

To finish, let's also investigate how well PCA performs as a data reduction method. By retaining only a limited number of principal components, we can build "reduced" versions of the images that involve only a number of principal components. @fig-pca-reduced shows two original faces from the dataset (left), together with compressed versions involving the first 10, 40, and 80 most significant principal components. The version that uses only 10 components is quite generic and it is difficult even to distinguish the male and female face. The version with 80 components, on the other hand, is very close to the original.

```{r}
#| fig-cap: Original images (left), and 3 PCA-reduced images with increasing numbers of principal components. 
#| label: fig-pca-reduced
project_data <- function(pc, n_retain) {
  t(t(pc$x[,1:n_retain] %*% t(pc$rotation)[1:n_retain,]) + pc$center)
}

par(mfrow=c(2, 4), mar=c(1, 1, 1, 1))
show_image(faces[,70], "Original")
show_image(project_data(pc_olivetti, 10)[,70], "10 PCs")
show_image(project_data(pc_olivetti, 40)[,70], "40 PCs")
show_image(project_data(pc_olivetti, 80)[,70], "80 PCs")

show_image(faces[,80], "Original")
show_image(project_data(pc_olivetti, 10)[,80], "10 PCs")
show_image(project_data(pc_olivetti, 40)[,80], "40 PCs")
show_image(project_data(pc_olivetti, 80)[,80], "80 PCs")

```

It is worth realizing the amount of data compression realized by using PCA. The original images had 4096 degrees of freedom, whereas the rightmost versions in @fig-pca-reduced are described by 80 loadings, more than a 50-fold reduction in degrees of freedom! Clearly there are some visual artifacts that appear in the compressed versions, but the faces are clearly distinguishable, and it seems very reasonable at this point that a machine learning algorithm (for example, to classify the faces, or to do segmentation) could take these compressed images as input and still perform well.
