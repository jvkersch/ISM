# Regression analysis

Often a lot of measurements for each subject (e.g., each animal or each plant) are collected in scientific experiments in biosciences. An ecologist could, for example, measure at the same time the number of a certain shrub on different plots of land, as well as the acidity of each plot in order to hopefully be able to describe a relationship between both. A biotechnologist might be interested in seeing which genes are important during which phase of the growth of a plant. He or she could therefore collect at appropriate times measurements for the expression of different genes and subsequently investigate the association between gene expression and time. The purpose of this chapter is to provide techniques to detect patterns and relations in complex datasets and then to use these relations to predict future outcomes. We will in particular focus on situations where we are interested in 1 specific continuous outcome and hope to understand its relationship with 1 or multiple continuous or qualitative variables.

## The linear regression model

Although the correlation coefficient is frequently used in explorative and descriptive statistics to describe an association between 2 continuous measurements, it has a number of limitations:

1. Its numerical value is difficult to interpret;
2. It cannot be used to predict the value of the outcome $Y$ (e.g., the number of nests of red land crabs in a certain area) based on some predictor value $X$ (e.g., the biomass of crabs in that area);
3. It does not allow for an easy correction of the association between the variables $X$ and $Y$ for the disturbing influence of measured confounders;
4. It does not allow for an easy verification of whether the strength of the association between the variables $X$ and $Y$ depends on the value of a third variable $Z$ (e.g., to verify if there exist gene-neighbourhood interactions where the influence that certain genes exert on the development of Chronic Obstructive Pulmonary Disease depends on smoking history);
5. It does not allow to describe nonlinear associations or associations between a continuous and a qualitative variable.

To handle these problems in a flexible way, we will use *regression techniques*

::: {.callout-note}
## Example: Woody debris and tree density
 
The human impact on freshwater environments concerns scientists a lot. Coarse woody debris (CWD) is fallen wood that provides a habitat for aquatic organisms and furthermore influences hydrological processes and the transport of organic materials within aquatic ecosystems. The presence of humans has altered the CWD input to aquatic systems.  Chistensen et al. (1996) therefore studied the connection between coarse woody debris and riparian vegetation in a sample of 16 North American lakes. They defined CWD as woody debris with a diameter larger than 5 cm and registered for a number of locations along the shoreline the CWD basal area (in m$^2$ per km of shoreline) and the tree density (in number per km of shoreline). To obtain a single measurement per lake, weighted averages were used.

The goal of this study is to describe the association between the tree density along the shoreline of the lake and the relative basal area of CWD. Since we want to explain the effect of the tree density on CWD, we call tree density the **explanatory**, **predictor** or **independent variable** and the CWD basal area the **outcome** or **dependent variable**, i.e., the variable in which we are primarily interested. For the rest of this section, we will always use $X$ for the independent variables and $Y$ for the dependent variables.

@fig-trees plots the CWD basal area (in m$^2$ per km) in function of the riparian tree density, together with a loess scatterplot smoother (dotted line). The plot gives no indication that the relation between both variables would not be linear. Hence the Pearson correlation coefficient is an appropriate measure. It is equal to 0.797, which suggests a strong increase in CWD basal area with an increased riparian tree density. We gain more insight in the strength of the association by studying the loess scatterplot smoother, since this function gives for every tree density value the expected outcome for the CWD basal area. Because this curve can be well approximated by a much simpler, linear relation, we also added the `best fitting' straight line (full line; i.e., the least squares regression line) to the plot. This gives an even clearer image of the relationship between both variables than the correlation coefficient, and also uses only 1 parameter (namely the slope) to do so. In this section we will see how to construct and interpret this so-called regression line.

```{r}
#| label: fig-trees
#| fig-align: center
#| fig-cap: "CWD basal area in function of tree density, with linear regression line (solid line) and loess scatterplot smoother (dashed line)."

trees <- read.csv("./datasets/01-regression/christ.csv")
plot(CWD.BASA ~ RIP.DENS, data = trees,
     xlab = "Riparian tree density",
     ylab = "CWD basal area")

m1 <- lm(CWD.BASA ~ RIP.DENS, data = trees)
abline(m1)

m2 <- loess(CWD.BASA ~ RIP.DENS, data = trees)
j <- order(trees$RIP.DENS)
lines(trees$RIP.DENS[j], m2$fitted[j], lty = "dashed")
```
:::

We denote with $E(Y|X=x)$ the mean outcome for the subgroup of the study population consisting of subjects for which the explanatory variable $X$ takes on the value $x$. For the CWD example above, $E(Y|X=1,200)$ indicates the mean CWD basal area per km of shoreline for lakes that have 1,200 trees per km along their shoreline. We would obtain this mean by registering, for all lakes in the study population with 1,200 trees per km of shoreline, the CWD basal area and then taking the mean of these values. The mean $E(Y|X=x)$ is called a *conditional mean* because it describes a mean outcome, conditional to the fact that $X=x$.

Now suppose that the mean outcome can be described linearly in function of the explanatory variable $X$, which means that
$$
    E(Y|X=x)=\alpha + \beta x,
$$ {#eq-linreg}
where $\alpha$ and $\beta$ are unknown numbers. In this expression, $E(Y|X=x)$ represents the value on the $Y$-axis, $x$ the value on the $X$-axis, the *intercept* $\alpha$ indicates the intersection with the $Y$-axis, and $\beta$ is the *slope* of the line. This expression is called a *statistical model*. This naming suggests that certain assumptions will be placed on the distribution of the observations. In particular it assumes that the mean outcome varies linearly in function of the predictor $X$. For this reason, this is also called a *simple linear regression model*. According to this model, every measurement $Y$ can be described, modulo an error term $\epsilon$, as a linear function of the explanatory variable $X$: 
$$
    Y=E(Y|X=x)+\epsilon=\alpha+\beta x+\epsilon,
$$
where $\epsilon$ represents the deviation between the observed outcome and its (conditional) mean value, i.e., the uncertainty in the response variable.

The parameters $\alpha$ and $\beta$ are unknowns. If we could observe the entire study population, we could determine both parameters exactly (by calculating for 2 $x$-values the mean outcome and then solve the resulting system of linear equations as given by @eq-linreg). In reality we only observe a limited sample from the study population and hence we need to estimate both parameters based on the available information. The parameters are estimated by searching for the line that `best fits' the data. For this we want that for a given value $x_i$ for subject $i$, the difference between the corresponding point on the regression line, $(x_i, \alpha + \beta x_i)$, and the observation itself, $(x_i, y_i)$, is as small as possible. This can be realised by choosing values for $\alpha$ and $\beta$ that minimise the sum of the squared distances between the predicted and the observed points:
$$
    \sum_{i=1}^n (y_i-\alpha-\beta x_i)^2.
$$
The obtained line is then called the *least squares (regression) line*. The corresponding values or estimations $\hat{\alpha}$ for $\alpha$ and $\hat{\beta}$ for $\beta$ are called the *least squares estimates*. It can be shown quite easily that 
$$
    \hat{\beta}=\frac{\mbox{Cor}(x,y)s_y}{s_x} 
$$
and that  
$$
    \hat{\alpha}=\bar y - \hat{\beta} \bar x.
$$
Note that the slope of the least squares line is proportional to the correlation between the outcome and the explanatory variable.

For the given estimates $\hat{\alpha}$ for $\alpha$ and $\hat{\beta}$ for $\beta$, the linear regression model [-@eq-linreg] allows us to do two things:

1. To predict the expected outcome for subjects with a given $x$-value for the explanatory variable as $\hat{\alpha}+\hat{\beta}x$.
2. To verify how much the outcome differs on average between 2 groups of subjects with a difference of $\delta$ units in the explanatory variable. This is:
$$
    E(Y|X=x+\delta)-E(Y|X=x) = \alpha + \beta (x+\delta) -\alpha-\beta x = \beta\delta.
$$
In particular, $\beta$ can be interpreted as the difference in mean outcome between 2 subjects that differ 1 unit in $X$-value. This difference can be estimated by $\hat{\beta}$.

::: {.callout-note}
## Woody debris and tree density, continued

An analysis of the data in R gives the following results:
```{r}
m <- lm(CWD.BASA ~ RIP.DENS, data = trees)
summary(m)
```

The software reports $\hat{\alpha}=-77.09908$ and $\hat{\beta}=0.11552$. We conclude that, per km of shoreline, the CWD basal area increases on average with 1.2 m$^2$ per increase of 10 trees in tree density. Furthermore, we can predict what CWD basal area can be expected for any given number of trees per km of shoreline. For example, if the tree density is 1,600 trees per km shoreline, we expect a mean CWD basal area of $-77.09908 +0.11552\times 1,600=108$ m$^2$ per km shoreline.

Remark, using @fig-trees, that the dataset does not contain any lakes with a tree density of approximately 1,600 trees per km of shoreline. Based on the dataset it would thus not be possible, without using a statistical model, to obtain an estimate for the mean CWD basal area for that given tree density. However, assuming that the mean CWD basal area varies linearly with the riparian tree density, we can use all\footnote{Indeed, this mean is calculated based on the estimates $\hat{\alpha}$ and $\hat{\beta}$, which were obtained using all observations.} observations to estimate this mean. Hence we obtain a meaningful and precise result, if the condition of linearity is met of course.
:::

```{r}
#| label: fig-residuals
#| fig-align: center
#| fig-cap: Residual plot for the CWD data.
m <- lm(CWD.BASA ~ RIP.DENS, data = trees)
plot(trees$RIP.DENS, m$residuals,
     xlab = "Riparian tree density",
     ylab = "Residuals")

lw <- loess(m$residuals ~ trees$RIP.DENS)
j <- order(trees$RIP.DENS)
lines(trees$RIP.DENS[j], lw$fitted[j])
```


For the results obtained from the linear regression model to be valid, it is important to verify that all conditions that are imposed by the model are met. So far, the only assumption we made was that the mean outcome varies linearly in function of the explanatory variable (later on, we will add more conditions to also determine the variability of the data around the regression line). This assumption can easily be verified graphically using a scatterplot where we plot the outcome in function of the explanatory variable and then check if the relation seems to follow a linear pattern. Deviations from linearity can usually be discovered more easily by means of a *residual plot*. This is a scatterplot with the explanatory variable on the $X$-axis and the *residuals* on the $Y$-axis. The residuals are the prediction errors that can be calculated as 
$$
e_i=y_i-\hat{y}_i=y_i-\hat{\alpha}-\hat{\beta}x_i.
$$
They represent the vertical distance between the observation for subject $i$ and its prediction on the regression line.

If the assumption of linearity holds, then there should be no pattern visible in the residual plot. This is the case in @fig-residuals that shows a residual plot for the regression analysis of the CWD example. However, when the residuals reveal a nonlinear pattern, this means that extra terms should be added to the model to correctly predict the mean outcome. For example, if the residuals show a quadratic pattern, then we could write that approximately $e_i\approx \delta_0+\delta_1 x_i+\delta_2 x_i^2$ for some numbers $\delta_0$, $\delta_1$, and $\delta_2$, and hence that the outcome $y_i=\hat{\alpha}+\hat{\beta}x_i+e_i\approx (\hat{\alpha}+\delta_0)+(\hat{\beta}+\delta_1)x_i+\delta_2 x_i^2$ (modulo an error term) is a quadratic function of $x_i$. In that case it is best to switch to a quadratic regression model.

Since the linearity of the model can only be verified over the observed range of the explanatory variable (for example, over the interval [771; 2,150] for the CWD data), it is important to understand that the results of a linear model cannot just be extrapolated past the largest or smallest observed $X$-value. In the CWD example we can estimate that the mean CWD basal area for lakes with a riparian tree density of 750 per km of shoreline will be $-77.09908 +0.11552\times 750=9.5$ m$^2$, but the observed data do not allow us verify the reliability of this estimation. After all, it could be that the regression line for low values of the predictor variable increases or decreases, causing the linear extrapolation to be misleading. Note that, for example, the prediction for a tree density of 500 per km of shoreline is very misleading, since it gives a negative result ($-77.09908 +0.11552\times 500=-19$ m$^2$).


::: {.callout-note}    
## Frequency of Lap94 and distance from Southport

The frequency of the allele Lap94 in Mytilus edulis (a mussel) with respect to the eastern distance from Southport, Connecticut, U.S.A., is shown in @fig-southport. A linear regression analysis shows that the expected gene frequency differs 2.6% between mussels that are located at a 10 km eastern distance from each other.

XXX use asin(sqrt(southport$freq))*180/pi transform? Sokal-Rohlf 1997.

```{r}
#| label: fig-southport
#| fig-align: center
#| fig-cap: "Frequency of Lap94 in function of the eastern distance from Southport, with linear regression line (dotted line), quadratic regression line (solid line), and loess scatterplot smoother (dashed line)"

southport <- read.csv("datasets/01-regression/southport.txt", sep = " ")
southport$freq <- southport$freq / 2

plot(freq ~ km, data = southport,
     xlab = "Distance (in km)",
     ylab = "Frequency")

# linear model
m_linear <- lm(freq ~ km, data = southport)
abline(m_linear, lty = "dotted")

# quadratic model
m_quad <- lm(freq ~ km + I(km^2), data = southport)
km_dense <- seq(0, 110, length.out = 100)
lines(km_dense, predict(m_quad, new = data.frame(km = km_dense)))

# loess model
m_loess <- loess(freq ~ km, data = southport)
lines(southport$km, m_loess$fitted, lty = "dashed")
```

```{r}
m_linear <- lm(freq ~ km, data = southport)
summary(m_linear)
```


\begin{figure}[htbp!]
\centering
\includegraphics[width=0.7\textwidth]%[height =  cm , width = 11cm]
{assets/eps/southport2.eps}
\caption{Residual plots for linear regression (left) and quadratic regression (right), with loess scatterplot smoother.}\label{fig:lap942}
\end{figure}

@fig-southport-residuals shows a residual plot based on linear regression. We see that that residuals display a systematic pattern, which is approximately parabolic. This suggests that adding a quadratic term to the model will improve the reliability of the regression model.

```{r}
#| label: fig-southport-residuals
#| fig-align: center
#| fig-cap: Residual plots for linear regression (left) and quadratic regression (right), with loess scatterplot smoother.

par(mfrow = c(1, 2))
plot(m_linear, which = 1, main = "Linear regression")
plot(m_quad, which = 1, main = "Quadratic regression")
```

Adding a quadratic term gives the following model:

```{r}
m_quad <- lm(freq ~ km + I(km^2), data = southport)
summary(m_quad)
```

We conclude from this that the mean frequency of the allele Lap$^{94}$ in Mytilus edulis can be described in function of the eastern distance $x$ from Southport as $(1.4-0.042x+0.0028x^2)/100$. @fig-southport-residuals (right) shows a residual plot based on this quadratic regression and indicates that the previously found pattern has mostly disappeared. Hence this model better describes the data. In subsequent sections it will be examined if the model can be improved further.

XXX Check numbers
:::

## The residual standard deviation

The CWD linear regression model from the previous section indicates how much (in particular, what basal area of) woody debris we can expect along North American lakes with a certain tree density. However, it does not inform us how much this area can vary between lakes with the same tree density. Nevertheless, it is of the utmost importance to know this when we want to make predictions based on the regression model, since outcomes that vary a lot around the regression line can of course not be predicted accurately by using the regression line, as opposed to outcomes with little variation around the line which can be predicted relatively accurately.

If the outcomes for lakes with the same predictor value $x$ (i.e., tree density) are normally distributed, then it makes sense to express the variation of the outcomes around their mean by means of a *conditional variance* $Var(Y|X=x)$. Similarly to the conditional mean $E(Y|X=x)$, this indicates the variance on the outcomes for the subgroup from the study population consisting of lakes for which the tree density $X$ takes on the value $x$. For example, $Var(Y|X=1,300)$ in the CWD example is the variance on the CWD basal area per km of shoreline for lakes with a riparian tree density of 1,300 per km. These variances cannot just be estimated, since there is only 1 observation in the dataset with a tree density of 1,300. When we examine @fig-residuals, we see that the points are equally spread around the regression line, irrespective of the tree density, and that the variability on the basal area of coarse woody debris does not seem to depend on the tree density. In this case, we call the outcomes *homoscedastic* or the variance is said to be *homogeneous*. It then makes sense to assume that the conditional variance $Var(Y|X=x)$ is constant: 
$$
    Var(Y|X=x)=\sigma^2.
$$ {#eq-homosc}
The constant $\sigma$ is called the *residual standard deviation*. If we assume @eq-homosc, we are able to determine the conditional variance $Var(Y|X=1,300)=\sigma^2$, because then it can be estimated based on the data for all lakes, as will be illustrated in the next paragraph.

As we have learned in descriptive statistics, the variation of the outcomes around their conditional mean can be described by means of the differences between the observations $y_i$ and their (estimated) mean $\hat{\alpha}+\hat{\beta}x_i$, or in other words, through the residuals. However, the mean of the residuals is always 0 (verify this!) because the positive and negative deviations cancel each other out. Hence the mean residual is not a good measure for the variation, and it is more sensible to use the squared deviations $e_i^2$. The mean of these *squared residuals* therefore will give a good measure. In particular it can be shown that the so-called \emph{residual mean squared error}
$$
    \hat{\sigma^2}=\frac{\sum_{i=1}^n e_i^2}{n-2}=s_y^2\{1-\mbox{Cor}(x,y)^2\}
$$ {#eq-mse}
is a good (i.e., unbiased) estimate of $\sigma^2$. Remark that the size of the residual mean squared error is closely connected with the correlation. If the outcome is independent from the predictor variable, the variability of the outcomes around the regression line is the same as the total variability, as denoted by the standard deviation $s_y$. If the variables $X$ and $Y$ are (perfectly) linearly dependent on each other, the correlation is 1, and hence there is no variation around the regression line. This is logical, since in that case the data points form a straight line and therefore do not vary around that line. 


::: {.callout-note}
## Woody debris and tree density, continued

Previously, the R model summary for the CWD model gave the following estimate for the residual standard deviation:

> Residual standard error: 36.32 on 14 degrees of freedom

Assuming the CWD basal area is normally distributed for a given tree density, we can conclude that respectively 68% and 95% of those basal areas given a tree density of 1,300 per km can be expected to fall in the intervals $[73-36, 73+36] = [37, 109]$ and $[73-2\times 36, 73 + 2\times 36] = [1, 145]$. Thus we obtain a fairly wide 95% reference interval for the CWD basal area when the tree density is 1,300 trees per km of shoreline. These intervals are not completely accurate since they do not take the imprecision of the estimates of the mean outcome and residual standard deviation into account. in the literature there exist so-called \emph{prediction intervals} which do take this imprecision into account.
:::

```{r}
#| label: fig-cwd-diagnostics
#| fig-align: center
#| fig-cap: "Analysis of the CWD data. Left: scatterplot of the squared residuals. Right: QQ-plot of the residuals."

par(mfrow = c(1, 2))
plot(m, which = 1)
plot(m, which = 2)
```

For the previous results to be valid, it is of course again important that all conditions imposed by the model are met. This time we do not only have the assumption of linearity, but more importantly also the assumption of homoscedasticity of the outcomes. Since, according to @eq-mse, the squared residuals are indicative for the variability that is present in the data, we can obtain knowledge concerning this assumption by making a scatterplot of the squared residuals (on the $Y$-axis) in function of the explanatory variable (on the $X$-axis).This is illustrated for the CWD data in @fig-cwd-diagnostics (left), which seems to suggest that the variability increases for increasing tree densities. Consequently, the reference intervals that have been calculated assuming homoscedasticity cannot be fully trusted. In particular they might be too narrow for high tree densities and too wide for low tree densities.

Even if the variance would be homogeneous, it is still important to verify that the outcomes are normally distributed for subjects with the same predictor value in order for the residual standard deviation to be a meaningful measure to describe the variability on the data and for the calculated reference intervals to be correct. A QQ-plot of the outcomes would be misleading, since this checks the normality of all measurements as a whole, not the normality of the measurements for subjects with the same predictor value. It can mathematically be shown that normally distributed outcomes for a given $x$-value implies that also the residuals approximately normally distributed. Hence deviations from normality in a QQ-plot for the residuals indicates therefore that the outcomes are not normally distributed for a fixed $x$. @fig-cwd-diagnostics (right) illustrates this for the CWD data and shows deviations from normality. This is not surprising, since heterogeneity of the variance often goes together with non-normality, in particular skewness, of the data.

Finally it is also necessary that all outcomes are independent to obtain correct estimations of the residual standard deviation. This would not be the case in so-called longitudinal studies where the outcome is measured repeatedly over time for the same subject (e.g., plants of animals).

In the next section we will describe how to handle deviations from the previously mentioned assumptions. 

## Deviations from the assumptions in linear regression analysis

The primary assumption in linear regression analysis is the assumption that the outcome varies linearly in the predictor. Whenever residual plots suggest that this is not the case, we could consider transforming the explanatory variable. In dose-response studies where, for example, the impact of increasing doses of a toxic substance on a phenotype in test animals is studied, the (mean) outcome will often not vary linearly in function of the administered dose, but will vary linearly in function of the logarithm of the administered dose. In that case, we could opt to include the log-transformed explanatory variable as a predictor in the model. For other examples it might happen that other transformations than the log-transformation are better suited, such as the square root ($\sqrt{x}$) or the inverse ($1/x$) transformation.

An advantage of transforming the explanatory variable is that it is easy to accomplish, a disadvantage is that this often complicates the interpretation of the coefficient in the model. However, this will not happen when applying a log-transformation, because an increase in log-dose with, for example, 1 unit is equivalent with a change in dose with a factor $\exp(1)=2.78$. Transforming the explanatory variable does not have a direct influence on the homogeneity of the variance or on the normality of the outcomes (for fixed values of the predictor variable), except by improving the linearity of the model. Therefore this option is often less suitable when there are strong deviations from normality. 

An alternative option to improve the linearity of the model, is *higher-order regression*. Here nonlinear relations are directly modeled by including higher-order terms in the model. We could, for example, consider a second-order model (see also Example~\ref{vb:lap94}): 
$$
    E(Y|X)=\alpha+\beta_1X+\beta_2X^2,
$$
in which case the regression curve will be parabolic, or a third-order model: 
$$
    E(Y|X)=\alpha+\beta_1X+\beta_2X^2+\beta_3X^3,
$$
where the regression curve will be a polynomial of degree 3. This method can be seen as some sort of transformation of the explanatory variable and essentially has the same properties, advantages, and disadvantages. However, an additional advantage is that in this case there is no need to decide yourself on a transformation, because the method itself will implicitly estimate a good polynomial.

Finally we could also consider to transform the outcome instead of the explanatory variable. For example, when the outcomes are right skewed, it is often appropriate to perform a log-transformation of the outcomes and include this new variable as the outcome variable in the model. Usually this not only improves the linearity of the model, but it will also improve the normality of the residuals with a more constant variability. This method has the same advantages and disadvantages as a transformation of the explanatory variable. A big difference between both options that greatly influences the choice between both methods is that, contrary to transformations of the outcome, transformations of the independent variable have little to no influence on the distribution of the residuals (unless via changes in their mean). Normally distributed residuals in particular will remain rather normally distributed after transforming the explanatory variable, whereas they might no longer be normally distributed after a transformation of the outcome variable, and vice versa.